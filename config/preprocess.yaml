
data_path: "data/origin"
out_path: "data/out"
tokenizer_path: "data/tokenizer"

vocab_size: 21128
preprocess: False